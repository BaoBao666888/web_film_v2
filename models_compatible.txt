Yêu cầu tương thích với code hiện tại:
- Định dạng GGUF
- Bản instruct/chat (có chat_template trong metadata)
- Chạy qua llama_cpp create_chat_completion

So sánh nhanh (ghi theo mức độ nhẹ → nặng, nhanh → chậm):
1) Qwen2.5-1.5B-Instruct (GGUF)
   - Rất nhẹ, rất nhanh, chat có VN khá
   - Chat/logic ổn với tác vụ đơn giản
2) Qwen2.5-7B-Instruct (GGUF)
   - Cân bằng chat/logic tốt hơn 3B
   - Chậm hơn 1.5B/3B, cần thêm RAM
3) Llama-3.1-8B-Instruct (GGUF)
   - Chat tự nhiên, general tốt
   - Chậm hơn 7B Qwen trên CPU
4) Mistral-7B-Instruct v0.3 (GGUF)
   - Tốc độ nhanh, chat ổn
   - Ít hợp với câu hỏi cần suy luận dài
5) Gemma-2-2B / Gemma-2-9B Instruct (GGUF)
   - 2B rất nhẹ, 9B tốt hơn nhưng chậm
   - Chat template rõ ràng, dễ thay thế
6) Phi-3.5-mini Instruct (GGUF)
   - Rất nhanh, nhẹ
   - Hay trả lời ngắn, kiến thức không sâu
7) Yi-1.5-6B / 9B Chat (GGUF)
   - Chat tự nhiên, VN khá
   - Tốc độ trung bình
8) Mixtral-8x7B-Instruct (GGUF)
   - Chat/logic tốt nhất trong danh sách
   - Rất nặng, rất chậm trên CPU

Ghi chú:
- Nếu GGUF không có chat_template, cần tự format prompt.
- LLAMA_N_CTX phụ thuộc model và RAM; set vừa phải để tránh chậm/hết bộ nhớ.
