from sentence_transformers import SentenceTransformer
import numpy as np
import joblib
import os

class CommentFilterEngine:
    def __init__(self, model_path="models/comment_filter.joblib"):
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ªõi file model
        base_dir = os.path.dirname(os.path.abspath(__file__))
        model_full_path = os.path.join(base_dir, model_path)

        print(f"üîπ Load comment filter model t·ª´: {model_full_path}")

        if not os.path.isfile(model_full_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y model: {model_full_path}")

        # Load d·ªØ li·ªáu model
        data = joblib.load(model_full_path)

        self.clf = data["clf"]
        self.label2id = data["label2id"]
        self.id2label = data["id2label"]
        base_model_name = data["base_model_name"]

        # Load encoder
        print(f"üîπ Load sentence-transformer base model: {base_model_name}")
        self.encoder = SentenceTransformer(base_model_name)

        # ‚ö†Ô∏è Ch·ªâ 3 nh√£n model: toxic | clean | spam
        self.clean_labels = {"clean"}
        self.toxic_labels = {"toxic", "spam"}

    def _predict_proba(self, emb):
        return self.clf.predict_proba(emb)

    def predict_one(self, text: str):
        """
        Ph√¢n lo·∫°i 1 b√¨nh lu·∫≠n v√† tr·∫£ th√™m is_toxic (k·∫øt h·ª£p model + rule)
        """
        # 1. Encode v√† l·∫•y x√°c su·∫•t
        emb = self.encoder.encode([text])
        probs = self._predict_proba(emb)[0]
        label_id = int(np.argmax(probs))
        label_name = self.id2label[label_id].lower()

        # 2. ƒê∆∞a probs v·ªÅ dict cho d·ªÖ x√†i
        probs_dict = {self.id2label[i]: float(p) for i, p in enumerate(probs)}
        toxic_prob = probs_dict.get("toxic", 0.0)
        spam_prob = probs_dict.get("spam", 0.0)

        # 3. Lu·∫≠t t·ª´ kh√≥a th·ªß c√¥ng (cho m·∫•y c√¢u ki·ªÉu "phim r·∫ª ti·ªÅn")
        text_lower = text.lower()
        manual_blacklist = [
            "r·∫ª ti·ªÅn",
            "phim r√°c",
            "r√°c ph·∫©m",
            "phim nh∆∞ c",
            "phim nh∆∞ cc",
            "nh∆∞ h·∫°ch",
            "nh∆∞ c·ª©t",
        ]
        rule_hit = any(kw in text_lower for kw in manual_blacklist)

        # 4. Logic quy·∫øt ƒë·ªãnh is_toxic
        #    - n·∫øu label != clean  -> toxic
        #    - ho·∫∑c prob toxic >= 0.30
        #    - ho·∫∑c prob spam  >= 0.30
        #    - ho·∫∑c tr√∫ng t·ª´ kh√≥a blacklist
        is_toxic = (
            label_name != "clean"
            or toxic_prob >= 0.8
            or spam_prob >= 0.40
            or rule_hit
        )

        return {
            "text": text,
            "label": label_name,           # clean / toxic / spam
            "confidence": float(probs[label_id]),
            "is_toxic": bool(is_toxic),    # ‚úÖ c·ªù cu·ªëi c√πng d√πng cho frontend
            "probs": probs_dict,
        }


    def predict_batch(self, texts):
        """
        Ph√¢n lo·∫°i list b√¨nh lu·∫≠n (√≠t d√πng)
        """
        emb = self.encoder.encode(texts, show_progress_bar=False)
        probs_all = self._predict_proba(emb)

        results = []
        for text, probs in zip(texts, probs_all):
            label_id = int(np.argmax(probs))
            label_name = self.id2label[label_id].lower()
            is_toxic = label_name not in self.clean_labels

            results.append({
                "text": text,
                "label": label_name,
                "confidence": float(probs[label_id]),
                "is_toxic": is_toxic,
                "probs": {self.id2label[i]: float(p) for i, p in enumerate(probs)},
            })

        return results


if __name__ == "__main__":
    engine = CommentFilterEngine()
    tests = [
        "phim hay qu√° c·∫£m ∆°n ad",
        "ƒëm up c√°i g√¨ v·∫≠y ƒë·ªì ngu",
        "v√†o nh√≥m zalo n√†y ki·∫øm ti·ªÅn n√®",
        "phim r√°c v√£i",
    ]
    for t in tests:
        print(engine.predict_one(t))
