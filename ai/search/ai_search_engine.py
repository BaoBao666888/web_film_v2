from sentence_transformers import SentenceTransformer
import numpy as np
import pickle
from rapidfuzz import fuzz
import os
import re
from underthesea import pos_tag


def clean_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"https?://\S+|www\.\S+", " ", s)
    s = re.sub(r"[^\w\s√Ä-·ªπ]", " ", s, flags=re.UNICODE)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def auto_query(raw: str) -> str:
    """
    T·ª± ƒë·ªông r√∫t keyword t·ª´ c√¢u t·ª± nhi√™n ti·∫øng Vi·ªát, nh∆∞ng KH√îNG l√†m m·∫•t keyword quan tr·ªçng.
    - POS tag ƒë·ªÉ l·∫•y danh t·ª´/danh t·ª´ ri√™ng/t√≠nh t·ª´.
    - N·∫øu k·∫øt qu·∫£ l·ªçc qu√° ng·∫Øn ho·∫∑c r·ªóng -> fallback d√πng c√¢u ƒë√£ clean.
    """
    q = clean_text(raw)
    if not q:
        return ""

    tagged = pos_tag(q)  # [(word, tag), ...]
    keep_tags = {"N", "Np", "A"}  # danh t·ª´, danh t·ª´ ri√™ng, t√≠nh t·ª´

    keywords = [w for (w, t) in tagged if t in keep_tags and len(w) > 1]
    kw = " ".join(keywords).strip()

    # Fallback th√¥ng minh: tr√°nh m·∫•t t√™n ri√™ng (vd: doraemon)
    if (not kw) or (len(kw.split()) <= 1 and len(q.split()) >= 3):
        return q

    return kw


class MovieSearchEngine:
    def __init__(self, model_path=None, index_path=None):
        base_dir = os.path.dirname(os.path.abspath(__file__))

        if model_path is None:
            model_path = os.path.join(base_dir, "models", "movie_semantic_vi")
        if index_path is None:
            index_path = os.path.join(base_dir, "data", "movie_index.pkl")

        print(f"üìÅ Model path: {model_path}")
        print(f"üìÅ Index path: {index_path}")

        # === Load model ===
        try:
            if os.path.isdir(model_path) and os.path.isfile(os.path.join(model_path, "config.json")):
                print(f"‚úÖ Load model fine-tune: {model_path}")
                self.model = SentenceTransformer(model_path)
            else:
                print("‚ö†Ô∏è Kh√¥ng th·∫•y fine-tune, d√πng model g·ªëc.")
                self.model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        except Exception as e:
            print("‚ö†Ô∏è L·ªói load model, fallback base.")
            print(e)
            self.model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

        # === Load index ===
        if not os.path.isfile(index_path):
            raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y index: {index_path} (h√£y ch·∫°y ai_build_index.py)")

        print(f"üîπ Load index phim t·ª´: {index_path}")
        with open(index_path, "rb") as f:
            data = pickle.load(f)

        self.embeddings = np.asarray(data["embeddings"], dtype="float32")
        self.ids = data["ids"]
        self.titles = data["titles"]
        self.texts = data["texts"]
        self.thumbnails = data.get("thumbnails", ["" for _ in range(len(self.ids))])
        self.posters = data.get("posters", ["" for _ in range(len(self.ids))])

        # precompute norms ƒë·ªÉ cosine nhanh v√† ·ªïn ƒë·ªãnh
        self.emb_norms = np.linalg.norm(self.embeddings, axis=1) + 1e-12

    def _semantic_scores(self, query: str):
        q_vec = self.model.encode(query)
        q_norm = np.linalg.norm(q_vec) + 1e-12
        sims = (self.embeddings @ q_vec) / (self.emb_norms * q_norm)  # [-1..1]
        return sims

    def _fuzzy_scores(self, query: str):
        # fuzzy match tr√™n title + full text ƒë·ªÉ ch·ªãu query t·ª± nhi√™n
        scores = np.zeros(len(self.titles), dtype="float32")
        q = (query or "").lower().strip()

        for i in range(len(self.titles)):
            title = (self.titles[i] or "").lower()
            full = (self.texts[i] or "").lower()

            s1 = fuzz.partial_ratio(q, title)
            s2 = fuzz.partial_ratio(q, full)
            scores[i] = max(s1, s2) / 100.0

        return scores  # [0..1]

    def _fuzzy_title_scores(self, query: str):
        # fuzzy ri√™ng cho title ƒë·ªÉ boost t√™n phim/franchise (vd: doraemon)
        q = (query or "").lower().strip()
        scores = np.zeros(len(self.titles), dtype="float32")
        for i, t in enumerate(self.titles):
            title = (t or "").lower()
            scores[i] = fuzz.partial_ratio(q, title) / 100.0
        return scores  # [0..1]

    def search(self, raw_query: str, top_k=10):
        raw = raw_query or ""
        q_auto = auto_query(raw)
        if not q_auto:
            return []

        q_low = q_auto.lower()
        q_tokens = q_low.split()

        # 1) semantic
        sem = self._semantic_scores(q_auto)
        sem01 = (sem + 1.0) / 2.0  # [0..1]

        # 2) fuzzy full (title + text)
        fz = self._fuzzy_scores(q_auto)  # [0..1]

        # 3) fuzzy title ri√™ng
        fz_title = self._fuzzy_title_scores(q_auto)  # [0..1]

        # === Heuristic t·ª± ƒë·ªông: query gi·ªëng "t√™n phim" hay "m√¥ t·∫£"? ===
        best_title = float(np.max(fz_title))
        is_title_like = (len(q_tokens) <= 3) or (best_title >= 0.85)

        # === title boost t·ª± ƒë·ªông (kh√¥ng keyword list) ===
        title_boost = np.zeros(len(self.titles), dtype="float32")
        for i, t in enumerate(self.titles):
            title = (t or "").lower()

            # query l√† substring title -> boost m·∫°nh
            if q_low and q_low in title:
                title_boost[i] = 0.35

            # fuzzy title c·ª±c cao -> boost v·ª´a
            if fz_title[i] >= 0.90:
                title_boost[i] = max(title_boost[i], 0.25)

        # === Hybrid score t·ª± ƒë·ªông ===
        if is_title_like:
            # Query gi·ªëng t√™n phim -> ∆∞u ti√™n fuzzy_title ƒë·ªÉ k√©o ƒë√∫ng title l√™n
            score = 0.25 * sem01 + 0.35 * fz + 0.40 * fz_title + title_boost
            thr = 0.30
        else:
            # Query gi·ªëng m√¥ t·∫£ -> semantic quan tr·ªçng h∆°n
            score = 0.65 * sem01 + 0.35 * fz + title_boost
            thr = 0.40

        # fallback: n·∫øu semantic y·∫øu, tƒÉng fuzzy t·ª± ƒë·ªông
        if float(np.max(sem01)) < 0.55:
            score = 0.40 * sem01 + 0.40 * fz + 0.20 * fz_title + title_boost
            thr = min(thr, 0.35)

        # L·∫•y top candidates
        top_idx = np.argsort(-score)[: max(top_k, 20)]

        results = []
        for i in top_idx:
            s = float(score[i])
            if s < thr:
                continue

            results.append(
                {
                    "id": self.ids[i],
                    "title": self.titles[i],
                    "score": s,
                    "semantic": float(sem01[i]),
                    "fuzzy": float(fz[i]),
                    "fuzzy_title": float(fz_title[i]),
                    "processed_query": q_auto,
                    "text": self.texts[i],
                    "thumbnail": self.thumbnails[i],
                    "poster": self.posters[i],
                }
            )

            if len(results) >= top_k:
                break

        return results


if __name__ == "__main__":
    engine = MovieSearchEngine()
    while True:
        q = input("Nh·∫≠p t·ª´ kh√≥a t√¨m phim (q ƒë·ªÉ tho√°t): ")
        if q.lower().strip() == "q":
            break

        res = engine.search(q, top_k=10)
        print("K·∫øt qu·∫£:")
        for r in res[:10]:
            print(
                f"- [{r['id']}] {r['title']} "
                f"score={r['score']:.3f} sem={r['semantic']:.3f} "
                f"fz={r['fuzzy']:.3f} fz_title={r['fuzzy_title']:.3f} "
                f"| processed='{r['processed_query']}'"
            )
        print()
